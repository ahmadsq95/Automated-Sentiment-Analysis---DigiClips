{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ylee_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ylee_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Necessary libraries\n",
    "import mysql.connector as mysql\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import sklearn\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "from  tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import sqlalchemy\n",
    "import pymysql\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Data\n",
    "\n",
    "    The function below gets the keywords from the Keywords table from the database. The reason for this is to search the SRT21 table with each keyword in this table to extract text that contains the keywords from the SRT21 table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: 8.0.28-0ubuntu0.20.04.3\n",
      "successful\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Submission_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mcdonalds</td>\n",
       "      <td>2022-03-05 18:58:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>walmart</td>\n",
       "      <td>2022-03-05 18:58:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Keyword     Submission_Time\n",
       "0   1  mcdonalds 2022-03-05 18:58:19\n",
       "1   2    walmart 2022-03-05 18:58:45"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Getting keywords table from database\n",
    "\n",
    "# enter your server IP address/domain name\n",
    "HOST = \"xxxxx\" # or \"domain.com\"\n",
    "# Database name, if you want just to connect to MySQL server, leave it empty\n",
    "DATABASE = \"xxxxxxx\"\n",
    "# This is the user you create\n",
    "USER = \"xxxxxxxxxxx\"\n",
    "# User password\n",
    "PASSWORD = \"xxxxxxxxxx\"\n",
    "# Connect to MySQL server\n",
    "db_connection = mysql.connect(host=HOST, database=DATABASE, user=USER, password=PASSWORD)\n",
    "print(\"Connected to:\", db_connection.get_server_info())\n",
    "# Fetch keywords\n",
    "query_keywords = \"SELECT * FROM xxxxxxxxxxx\"\n",
    "keywords = pd.read_sql_query(query_keywords, db_connection)\n",
    "\n",
    "print(\"successful\")\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The function below pulls the most recent day(i.e yesterday) of SRT21 table. The number of most recent days can be changed by altering the query below. Using pandas library, the data table is converted to a data frame so we can process the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: 8.0.28-0ubuntu0.20.04.3\n",
      "successful\n"
     ]
    }
   ],
   "source": [
    "## Getting the most recent data\n",
    "\n",
    "# Enter your server IP address/domain name\n",
    "HOST = \"xxxxxxxxxx\" # or \"domain.com\"\n",
    "# Database name, if you want just to connect to MySQL server, leave it empty\n",
    "DATABASE = \"xxxxxxx\"\n",
    "# This is the user you create\n",
    "USER = \"xxxxxxxxxx\"\n",
    "# User password\n",
    "PASSWORD = \"xxxxxxxxx\"\n",
    "# Connect to MySQL server\n",
    "db_connection = mysql.connect(host=HOST, database=DATABASE, user=USER, password=PASSWORD)\n",
    "print(\"Connected to:\", db_connection.get_server_info())\n",
    "\n",
    "# Query to fetch SRT21 table\n",
    "querystring = \"SELECT * FROM xxxxxxx where Created_at >= DATE_SUB(NOW(),INTERVAL 1 day)\"\n",
    "SRTtable = pd.read_sql_query(querystring, db_connection)\n",
    "print(\"successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From None type to Dataframe\n",
    "SRTtable = pd.DataFrame(SRTtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56685 entries, 0 to 56684\n",
      "Data columns (total 7 columns):\n",
      "Channel_Id       56685 non-null int64\n",
      "Mp4_File_Name    56685 non-null object\n",
      "CC_Num           56685 non-null int64\n",
      "Timecode         56685 non-null object\n",
      "Created_at       56685 non-null datetime64[ns]\n",
      "Finished_at      56685 non-null datetime64[ns]\n",
      "Cat_Frames       56685 non-null object\n",
      "dtypes: datetime64[ns](2), int64(2), object(3)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "SRTtable.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        -(bleats) -Axel! I knew you’d come back for me...\n",
       "1        -Axel! I knew you’d come back for me. Oh, I lo...\n",
       "2        with your wet slimy kisses. (screams) Ew, slug...\n",
       "3        Hmm. Oh, right. I’ve been ditched, I’m lost, a...\n",
       "4        Satanists. (grunting) Let us out! We’ll do Sat...\n",
       "                               ...                        \n",
       "56680    ARRIBA EL EQUIPO DE  CONTENDIENTES APROVECHAND...\n",
       "56681    DE AYER.. PARA ALFREDO. UNOS 56 1 A 0 LA PIZAR...\n",
       "56682    EQUIPO DE CONTENDIENTES A  CONTINUACIóN LA SEG...\n",
       "56683    CONCENTRADAS LISTAS 3 2 1 BAJA LA BARRA DE INI...\n",
       "56684    PIE DERECHO EL EQUIPO DE LOS  CONTENDIENTES ES...\n",
       "Name: Cat_Frames, Length: 56685, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRTtable['Cat_Frames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "    The data cleaning phase involves removing missing data, converting letters to lower-case letters only, removing punctuation, and keeping only English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation and stopwords\n",
    "def process(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]  # Remove stopwords\n",
    "    words = \"\"\n",
    "    for i in text:  #word stemming\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to remove non-ascii characters. \n",
    "def cleandata(text):\n",
    "    \n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "   \n",
    "    # Remove any non ASCII characters\n",
    "    encoded = text.encode('ascii', errors='ignore')\n",
    "    decode = encoded.decode()\n",
    "   \n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the data cleaning functions\n",
    "SRTtable['Cat_Frames'] = SRTtable['Cat_Frames'].fillna(' ')\n",
    "SRTtable['Cat_Frames'] = SRTtable['Cat_Frames'].apply(process)\n",
    "SRTtable['Cat_Frames'] = SRTtable['Cat_Frames'].apply(cleandata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which lines throws errors because they're empty\n",
    "text = SRTtable['Cat_Frames']\n",
    "langdet = []                                       \n",
    "indexError = []\n",
    "\n",
    "for i in range(len(SRTtable)):\n",
    "    try:\n",
    "        lang = detect(text[i])\n",
    "        langdet.append(lang)\n",
    "    except:\n",
    "        indexError.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop index with errors\n",
    "for i in indexError:\n",
    "    SRTtable = SRTtable.drop(i, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the ones that are English since we are only working with English for now\n",
    "SRTtable['is_english'] = SRTtable['Cat_Frames'].apply(lambda x: detect(x) == 'en')\n",
    "SRTtable = SRTtable[SRTtable.is_english == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRTtable['Cat_Frames']=SRTtable['Cat_Frames'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             bleat axel knew youd come back oh love goat \n",
       "1        axel knew youd come back oh love goat wet slim...\n",
       "2        wet slimi kiss scream ew slug ew ew ew ew ew e...\n",
       "3        hmm oh right iv ditch im lost peopl look satan...\n",
       "4        satanist grunt let us well satan stuff like gr...\n",
       "                               ...                        \n",
       "56605    album jason thrill fan iheart radio theater re...\n",
       "56606    macon georgia it alway fun play especi new son...\n",
       "56607    set set song apart think feel it one big recor...\n",
       "56608    it one big record 20 new song ail bull ten liv...\n",
       "56609    never put greatest hit record live album anyth...\n",
       "Name: Cat_Frames, Length: 40569, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRTtable['Cat_Frames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preparation\n",
    "    After cleaning the data, we need to prepare the data for the sentiment analysis. That means in order to get text related to the key word, we first need to group the data by channel because text in context can only be from one channel. Moreover, the SRT21 table's Cat_Frames column has overlapping text which means half the text in row 1 repeats in row 2 and so on. The reason for it is to ensure that data leaking does not happen. The overlapping causes a dublication in data, which has to be removed to continue getting context from text and getting more accurate sentiment results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make the Cat Frames column into a list\n",
    "def createList(conv_df):\n",
    "    cclist = []\n",
    "    # Iterate over each row\n",
    "    for index, rows in conv_df.iterrows():\n",
    "        # Create list for the current row\n",
    "        my_list =[rows.Cat_Frames]\n",
    "        # append the list to the final list\n",
    "        cclist.append(my_list)\n",
    "        \n",
    "    print(cclist[:10])\n",
    "    \n",
    "    return cclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in the new list and removes the overlapping text in each row\n",
    "def removeOverlap(cclist):\n",
    "    \n",
    "    try:\n",
    "        for sentence in range(len(cclist)):\n",
    "            # Set the two strings to elements in the list.\n",
    "            string1 = cclist[sentence]\n",
    "            string2 = cclist[sentence + 1]\n",
    "\n",
    "            # If the strings are a perfect match, drop one.\n",
    "            if string1 == string2:\n",
    "                cclist.remove(string2)\n",
    "\n",
    "            else:\n",
    "                # Convert to type string.\n",
    "                string1 = ' '.join(string1)\n",
    "                string2 = ' '.join(string2)\n",
    "\n",
    "                # Split into a word list.\n",
    "                str1array = string1.split()\n",
    "                str2array = string2.split()\n",
    "\n",
    "                # Find the the last word of string1 in string2 and note the index.\n",
    "                # Use that index to make a new string2.\n",
    "                if str1array[-1] in str2array:\n",
    "                    index = str2array.index(str1array[-1])\n",
    "                    string2 = str2array[index + 1:]\n",
    "                    string2 = ' '.join(string2)\n",
    "                    cclist[sentence + 1] = string2\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    # For when the index is out of range.\n",
    "    except IndexError:\n",
    "        print(\"end of list\")\n",
    "        \n",
    "        return cclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the list back into the Cat Frames column\n",
    "def replaceCatFrames(conv_df, cclist):\n",
    "    \n",
    "    conv_df['Cat_Frames'] = cclist\n",
    "    \n",
    "    return conv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bleat axel knew youd come back oh love goat '], ['axel knew youd come back oh love goat wet slimi kiss scream ew slug ew ew ew ew ew ew '], ['wet slimi kiss scream ew slug ew ew ew ew ew ew hmm oh right iv ditch im lost peopl look '], ['hmm oh right iv ditch im lost peopl look satanist grunt let us well satan stuff '], ['satanist grunt let us well satan stuff like grow pointi beard draw eyelin ask final time sacrific '], ['draw eyelin ask final time sacrific told dont know goat prepar die '], ['grunt yawn time happen oh sweeti got littl overexcit barf sleep wait spent night sleep bed '], ['center spoon doesnt get child wait there still time us genderblind product crucibl whatev word mean meh that kind jule thing right id kind rather stay bed '], ['whatev word mean meh that kind jule thing right id kind rather stay bed lisa three mm celtic heavi metal music play '], ['lisa three mm celtic heavi metal music play dont need stupid martin make ankl rock compass way go ']]\n",
      "end of list\n"
     ]
    }
   ],
   "source": [
    "# Remove the overlapping text\n",
    "cclist = createList(SRTtable)\n",
    "finalTextList = removeOverlap(cclist)\n",
    "SRTtable = replaceCatFrames(SRTtable, finalTextList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bleat axel knew youd come back oh love goat '], 'wet slimi kiss scream ew slug ew ew ew ew ew ew', ['wet slimi kiss scream ew slug ew ew ew ew ew ew hmm oh right iv ditch im lost peopl look '], 'satanist grunt let us well satan stuff', ['satanist grunt let us well satan stuff like grow pointi beard draw eyelin ask final time sacrific '], 'told dont know goat prepar die', ['grunt yawn time happen oh sweeti got littl overexcit barf sleep wait spent night sleep bed '], '', ['whatev word mean meh that kind jule thing right id kind rather stay bed lisa three mm celtic heavi metal music play '], ['lisa three mm celtic heavi metal music play dont need stupid martin make ankl rock compass way go ']]\n"
     ]
    }
   ],
   "source": [
    "print(finalTextList[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRTtable['Cat_Frames']=SRTtable['Cat_Frames'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRTtable = SRTtable.drop_duplicates(subset=['Cat_Frames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRTtable['Cat_Frames'] = SRTtable['Cat_Frames'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by channel\n",
    "grouped_Channel = SRTtable.groupby(SRTtable.Channel_Id)\n",
    "Channel_ID_List = SRTtable['Channel_Id'].unique()\n",
    "keywordList = keywords['Keyword'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Conversation function\n",
    "    This function gets keyword-related rows from SRT21 table. Because of text that is about a keyword, it is not nassesary to contain the keyword. Also, getting only rows contain the keyword is inefficient because we can not retrive enough data to perform the sentiment analysis. Therefore, we found an approach to get as much related rows by searching for keyword in rows. Then see the time gap between two keyword. If there is another hit of the keyword in less than 2 minutes, we cosider all rows between these two hits of the keyword as one conversation. On the other hand, if only one hit of the keyword in the two minutes length, we only consider 30 seconds length as a conversation. Programmically, the function takes data frame from SRT21 table with only one channel data in it, then split timecode column into start time and end time columns to use it later on to check times between keyword hits. Then, it search for all hit in the dataframe and put it in a new dataframe. Then, it goes through all hits in the new dataframe and check for times between hits, if the time is more than 2 minutes, it will get the previous and next row of the hit row from the original dataframe using indexes. Also, if the time between two hits is less than two minutes, it will consider all text between these two hits are in same context and it will get all rows in between to the new dataframe. Surely, the accuracy of capturing the conversaion would not be great. Meaning that some in-context would not be captured, and some out-of-context text would be include in \"conversation\" text. To increase the accuracy and improve the detecting conversations, Machine Learning algorarithms must be involved to recognize start and end of a conversation by learning when a conversation or a dialoage would start and conclude.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting keyword hits and needed rows that may related to the keywords(i.e a conversation)\n",
    "# Df is dataframe of one channel \n",
    "def get_conversation(df):\n",
    "    \n",
    "    # Split the two times in timecode column into their own columns\n",
    "    df[['StartTime', 'EndTime']] = df['Timecode'].str.split(',', expand=True)\n",
    "    # Create index column\n",
    "    df = df.reset_index()\n",
    "    conv_id = 0\n",
    "    # Get keyword and search for it in the df\n",
    "    # Put keyword hits into a new df\n",
    "    for keyword in keywordList:\n",
    "        for row, index in df.iterrows():\n",
    "            newdf = df[df['Cat_Frames'].str.contains(keyword)]\n",
    "            newdf['keyword'] = keyword\n",
    "           \n",
    "        for row, index in newdf.iterrows():\n",
    "            # Set format of the timecode column\n",
    "            date_format_str = '%H:%M:%S.%f'\n",
    "            minutes = 2\n",
    "\n",
    "            for index in range(len(newdf)):\n",
    "                newdf['Conversation_ID'] = conv_id + 1\n",
    "                # Initialize variables for the times\n",
    "                try:\n",
    "                    nextIndex = index + 1\n",
    "\n",
    "                    initial_HitStart = newdf.loc[newdf.index[index], 'StartTime']\n",
    "                    initial_HitEnd = newdf.loc[newdf.index[index], 'EndTime']\n",
    "\n",
    "                    next_HitStart = newdf.loc[newdf.index[nextIndex], 'StartTime']\n",
    "                    next_HitEnd = newdf.loc[newdf.index[nextIndex], 'EndTime']\n",
    "\n",
    "                    # Set to time format\n",
    "                    initial_start_time = datetime.strptime(initial_HitStart, date_format_str)\n",
    "                    initial_end_time = datetime.strptime(initial_HitEnd, date_format_str)\n",
    "\n",
    "                    next_start_time = datetime.strptime(next_HitStart, date_format_str)\n",
    "                    next_end_time = datetime.strptime(next_HitEnd, date_format_str)\n",
    "\n",
    "                    # Add 2 minutes to the time\n",
    "                    final_initialStart_time = initial_start_time + timedelta(minutes = minutes)\n",
    "                    final_initialEnd_time = initial_end_time + timedelta(minutes = minutes)\n",
    "\n",
    "                    # Check if the time of the next hit is 2 minutes after the first hit ends\n",
    "                    if next_start_time == final_initialEnd_time:\n",
    "                        # Check for the rows' index in the old dataframe\n",
    "                        # Add indices to a list\n",
    "                        location = []\n",
    "                        location.append(newdf.loc[newdf.index[index], 'index'])\n",
    "                        location.append(newdf.loc[newdf.index[nextIndex], 'index'])\n",
    "                        index_range = range(location[0] + 1, location[1])\n",
    "                        index_list = list(index_range)\n",
    "                        # Add inbetween rows to new dataframe\n",
    "                        for position in index_list:\n",
    "                            try:\n",
    "                                newdf.append(df.iloc[position])\n",
    "                            except (ValueError, IndexError):\n",
    "                                continue\n",
    "\n",
    "                    # If the rows are more than 2 minutes apart, do the same process but get the row above and below the initial row only\n",
    "                    else:\n",
    "                        location = []\n",
    "                        location.append(newdf.loc[newdf.index[index], 'index'])\n",
    "                        for position in location:\n",
    "                            try:\n",
    "                                newdf.append(df.iloc[position + 1])\n",
    "                                newdf.append(df.iloc[position - 1])\n",
    "                            except (ValueError, IndexError):\n",
    "                                continue\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis function\n",
    "\n",
    "    The function below takes data drame with only text in context that associated to a keyword and apply sentiment test. The sentiment analysis contains two type of test. The first test called polarity test which gives a score of how positive or negative is the text. Moreover, it contains 3 scores from 0 to 1 a score for negativity of the text, the neturality of the text, and the positivity of the text. The other test called subjectivity test, it is test the text and gives a score for that tells if the text is based on opinion or facts, it test the language used in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(final_df):\n",
    "    report = pd.DataFrame()\n",
    "    report['keyword'] = final_df['keyword'].astype(str)\n",
    "    report['Conversation_ID'] = 1\n",
    "    report['Channel_Id'] = final_df['Channel_Id'].astype(str)\n",
    "    report['Mp4_File_Name'] = final_df['Mp4_File_Name'].astype(str)\n",
    "    report['CC_Num'] = final_df['CC_Num'].astype(str)\n",
    "    report['Timecode'] = final_df['Timecode'].astype(str)\n",
    "    report['Created_at'] = final_df['Created_at'].astype(str)\n",
    "    report['Finished_at'] = final_df['Finished_at'].astype(str)\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    report['polarity_scores'] = final_df['Cat_Frames'].apply(lambda Cat_Frames: sid.polarity_scores(Cat_Frames))\n",
    "    report['compound'] = final_df['Cat_Frames'].apply(lambda Cat_Frames: sid.polarity_scores(Cat_Frames)['compound'])\n",
    "    report['comp_score'] = report['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')\n",
    "    report['SubjScores'] = final_df['Cat_Frames'].apply(lambda Cat_Frames: TextBlob(Cat_Frames).sentiment.subjectivity)\n",
    "    report['Subjectivity'] = report['SubjScores'].apply(lambda c: 'Subjective' if c >=0.5 else 'Objective')\n",
    "\n",
    "    report['polarity_scores'] = report['polarity_scores'].astype(str)\n",
    "    report['compound'] = report['compound'].astype(str)\n",
    "    report['SubjScores'] = report['SubjScores'].astype(str)\n",
    "    report['comp_score'] = report['comp_score'].astype(str)\n",
    "    report['Subjectivity'] = report['Subjectivity'].astype(str)\n",
    "\n",
    "    report['Conversation_ID'] = report['Conversation_ID'].astype(str)\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the results to the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(df):\n",
    "    database_username = 'xxxxxxxxx'\n",
    "    database_password = 'xxxxxxxx'\n",
    "    database_ip       = 'xxxxxxxx'\n",
    "    database_name     = 'xxxxxxxx'\n",
    "    database_connection = sqlalchemy.create_engine('mysql+mysqlconnector://{0}:{1}@{2}/{3}'.\n",
    "                                               format(database_username, database_password, \n",
    "                                                      database_ip, database_name), pool_recycle=1, pool_timeout=57600).connect()\n",
    "    df.to_sql(con=database_connection, name='xxxxxxxxx', if_exists='append',index= False)\n",
    "    database_connection.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here where the magic happens\n",
    "\n",
    "    After Cleaning phase and grouping the clean data by Channels, the goal is sending every channel dataframe to get_conversation function and then perform the sentiment analysis and write the result back to the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ylee_\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\ylee_\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# looping through channels to create dataframes for each channel and send each dataframe to get_conversation function\n",
    "# conv_df supposed to contain only data frame of desired conversation\n",
    "count = 0\n",
    "for i in range(len(Channel_ID_List)):\n",
    "    ## get df of one channel as df\n",
    "    df = grouped_Channel.get_group(Channel_ID_List[i])\n",
    "    ## get conversation from channel df\n",
    "    conv_df = get_conversation(df)   \n",
    "    ## perform sentiment analysis on conversation df\n",
    "    report_df = sentiment_analysis(conv_df)\n",
    "    # write result to db\n",
    "    write_to_db(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "To conclude, this project obtain, clean, and extraact conversaions from TV recoreded subtitles. on personal computers, this file takes around 50 minutes to run SRT21 table for one day. We recommend to improve capturing the context of text by investigating how to indicate conversations' start and end. Also, it is possible to use supervised machile leaning algorathims to automate the process. by feeding the model with labling text with thier keyword to train the model.  \n",
    "\n",
    "\n",
    "At the end, we, Yvonne and Ahmad, thank Digiclips and Christy Pearson for the opprtonity to work on a real-life project. This project gave us new skills and taought us new frontiers in data Science. We wish Digiclips the best in thier business and we hope that we were helpful. Also, we apreciate Ms. Pearson for her guidnece and support throughout the semester."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42588fd11209419b7c81abe23e31a1d2101f60811b02170c268e57ebb57d5d9a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
